{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Task2/3_Multiclass_classification_of_NFR_subclasses.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-djxLyuc-Opk"
      },
      "source": [
        "# Prepare\n",
        "Install required libraries and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Epk5taxa99eI",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers fastprogress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fr6bTWdl-XzF",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rdgM39FGZpM",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.multiclass import unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpZkAwZl0DaA",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
        "from pytorch_transformers import AdamW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L57NdoEnLQa2",
        "colab": {}
      },
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azCEB4CuQk9A"
      },
      "source": [
        "Check, if and what kind of GPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wtzha3q7QjjU",
        "colab": {}
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "if cuda_available:\n",
        "    curr_device = torch.cuda.current_device()\n",
        "    print(torch.cuda.get_device_name(curr_device))\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl2KDTqN6zLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device)/1000000\n",
        "\n",
        "def get_memory_usage_str():\n",
        "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOKXVgJgGtYV"
      },
      "source": [
        "Create a config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0lgLyC6Gsnf",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "class Fold(Enum):\n",
        "  No = 1\n",
        "  TenFold = 2\n",
        "  ProjFold = 3\n",
        "\n",
        "config = Config(\n",
        "    num_labels = 4, # will be set automatically\n",
        "    model_name=\"bert-base-cased\", \n",
        "    max_lr=2e-5, # default: 2e-5\n",
        "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
        "    epochs=16,\n",
        "    bs=16, # default: 2 or 4\n",
        "    weight_decay = 0.01,\n",
        "    max_seq_len=128,\n",
        "    train_size=0.75,\n",
        "    loss_func=nn.CrossEntropyLoss(),\n",
        "    seed=904727489, #default: 904727489, 42 (as in Dalpiaz) or None\n",
        "    es = False,\n",
        "    min_delta = 0.01,\n",
        "    fold = Fold.TenFold # Fold.No, Fold.TenFold, Fold.ProjFold\n",
        ")\n",
        "\n",
        "clazz = 'clazz'\n",
        "\n",
        "config_data = Config(\n",
        "    root_folder = '.',\n",
        "    data_folder = '/',\n",
        "    train_data = ['Raw-DataTrack-Huang_all.csv'],\n",
        "    label_column = clazz,\n",
        "    log_file = '/log/' + clazz + '_' + Fold(config.fold).name + '_classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        "    result_file = '/log/' + clazz + '_' + Fold(config.fold).name +  '_classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        "    model_path = '/model/',\n",
        "    model_name = 'NoRBERT.pkl',\n",
        "    #project_fold = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 12, 15], [2, 5, 11], [6, 9, 14], [7, 8, 13], [2, 4, 15], [4, 7, 10] ],\n",
        "    project_fold = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15] ],\n",
        "    #classes = ['US', 'SE', 'O', 'PE'],\n",
        "    classes= ['A', 'FT', 'L', 'LF', 'MN', 'O', 'PE', 'PO', 'SC', 'SE', 'US'],\n",
        ")\n",
        "\n",
        "load_from_gdrive = True\n",
        "save_model = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVU_viFX-ezy"
      },
      "source": [
        "To import the dataset, first we have to connect to our Google drive (if data should be loaded from gdrive). For this, we have to authenticating the access and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmGISBrhW-VJ",
        "colab": {}
      },
      "source": [
        "if load_from_gdrive:\n",
        "    from google.colab import drive\n",
        "    # Connect to drive to load the corpus from there\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    config_data.root_folder = '/content/drive/My Drive/x/Dataset/Promise'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "er1yzLHFQq1U",
        "colab": {}
      },
      "source": [
        "def initLog():\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
        "    with open(logfile, 'w') as log:\n",
        "        log.write(log_txt + '\\n')\n",
        "\n",
        "def logLine(line):\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(line + '\\n')\n",
        "\n",
        "def logResult(result):\n",
        "    logfile = config_data.root_folder + config_data.result_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(get_info() + '\\n')\n",
        "        log.write(result + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE5NcUY6_Hja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_info():\n",
        "    model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, train_size: {}, weight decay: {}, Seed: {}, Data: {}, Column: {}, EarlyStopping: {}:{}'.format(config.model_name, config.max_lr, config.epochs, config.bs, config.train_size, config.weight_decay, config.seed, config_data.train_data, config_data.label_column, config.es, config.min_delta)\n",
        "    return model_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIOWiuX4y85P",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "    if seed is None:\n",
        "        seed = random.randint(0, 2**31)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    return seed\n",
        "\n",
        "set_seed(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ptp6NhIC_FQb"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JmN-0wJEBAEH"
      },
      "source": [
        "Create proper tokenizer for our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6anB63ppBAtB",
        "colab": {}
      },
      "source": [
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str):\n",
        "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1G8rFbEEJWyu"
      },
      "source": [
        "Now, we can create our own databunch using the tokenizer above. Notice we're passing the include_bos=False and include_eos=False options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens.\n",
        "\n",
        "We can pass our own list of Preprocessors to the databunch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNRRj6jIJrp2",
        "colab": {}
      },
      "source": [
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]\n",
        "\n",
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZESEaXBOuNfn",
        "colab_type": "text"
      },
      "source": [
        "Create the BertTextClassifier-Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_PRt9Q3eUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertTextClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        super(BertTextClassifier, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        #self.apply(self.init_weights)\n",
        "    \n",
        "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
        "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        # According to documentation of pytorch-transformers, pooled output might not be the best \n",
        "        # and youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence \n",
        "        #hidden_states = outputs[0]\n",
        "        #pooled_output = torch.mean(hidden_states, 1)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(dropout_output)\n",
        "\n",
        "        activation = nn.Softmax(dim=1)\n",
        "        probs = activation(logits)   \n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IwxQFKykzpQq"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TWP1X17N5tJx",
        "colab": {}
      },
      "source": [
        "def create_label_indices():\n",
        "    #prepare label\n",
        "    labels = config_data.classes\n",
        "    labels.append('Other')\n",
        "  \n",
        "    #create dict\n",
        "    labelDict = dict()\n",
        "    for i in range (0, len(labels)):\n",
        "        labelDict[i] = labels[i]\n",
        "    return labelDict\n",
        "\n",
        "label_indices = create_label_indices()\n",
        "print(label_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeaTvNRTypP0",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "    fpath = config_data.root_folder + config_data.data_folder + filename\n",
        "    print(fpath)\n",
        "    df = pd.read_csv(fpath, delimiter=';', header=0, encoding='utf8', names=['number', 'ProjectID', 'RequirementText', 'clazz', 'NFR', 'F', 'A', 'FT', 'L', 'LF', 'MN', 'O', 'PE', 'PO', 'SC', 'SE', 'US'])\n",
        "    df = df.dropna()\n",
        "    is_NFR = df['NFR']==1\n",
        "    df = df[is_NFR]\n",
        "    \n",
        "    inv_map = {v: k for k, v in label_indices.items()}\n",
        "    df[config_data.label_column] = df[config_data.label_column].map(inv_map)\n",
        "    df[config_data.label_column].fillna(inv_map.get('Other'), inplace=True)\n",
        "    df[config_data.label_column]=df[config_data.label_column].astype(int)\n",
        "    df = df.loc[df[config_data.label_column] != 7]\n",
        "    return df\n",
        "\n",
        "def load_all_data(filenames):\n",
        "    df = load_data(filenames[0])\n",
        "    for i in range(1, len(filenames)):\n",
        "        df = df.append(load_data(filenames[i]))\n",
        "    return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6o6UU0tUYck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the train datasets\n",
        "df = load_all_data(config_data.train_data)\n",
        "\n",
        "# shuffle the dataset a bit and get the amount of classes\n",
        "df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
        "config.num_labels = df[config_data.label_column].nunique()\n",
        "\n",
        "print(df.shape)\n",
        "print(df[config_data.label_column].value_counts())\n",
        "print(df['ProjectID'].value_counts())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SClv488eQC8B"
      },
      "source": [
        "# Predictor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prtJ_TGhC2PO"
      },
      "source": [
        "Create a predictor class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qubb_Ka-C78O",
        "colab": {}
      },
      "source": [
        "class Predictor:\n",
        "    def __init__(self, classifier):\n",
        "        self.classifier = classifier\n",
        "        self.classes = self.classifier.data.classes\n",
        "\n",
        "    def predict(self, text):\n",
        "        prediction = self.classifier.predict(text)\n",
        "        prediction_class = prediction[1]\n",
        "        return self.classes[prediction_class]   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyVQS13d5Sft"
      },
      "source": [
        "# Create and train the learner/classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GeTaowHZzrSJ"
      },
      "source": [
        "Create the needed functions to create and train a classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T83UogVz5XJJ",
        "colab": {}
      },
      "source": [
        "def split_dataframe(df, train_size = 0.8, random_state = None):\n",
        "    # split data into training and validation set\n",
        "    df_trn, df_valid = train_test_split(df, stratify = df[config_data.label_column], train_size = train_size, random_state = random_state)\n",
        "    return df_trn, df_valid\n",
        "  \n",
        "def create_databunch(config, df_trn, df_valid):\n",
        "    bert_tok = BertTokenizer.from_pretrained(config.model_name,)\n",
        "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
        "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
        "    return BertDataBunch.from_df(\".\", \n",
        "                   train_df=df_trn,\n",
        "                   valid_df=df_valid,\n",
        "                   tokenizer=fastai_tokenizer,\n",
        "                   vocab=fastai_bert_vocab,\n",
        "                   bs=config.bs,\n",
        "                   text_cols='RequirementText',\n",
        "                   label_cols=config_data.label_column,\n",
        "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "              )\n",
        "\n",
        "\n",
        "def create_learner(config, databunch):\n",
        "    model = BertTextClassifier(config.model_name, config.num_labels)\n",
        "\n",
        "    optimizer = partial(AdamW)\n",
        "    if config.es:\n",
        "      learner = Learner(\n",
        "        databunch, model,\n",
        "        optimizer,\n",
        "        wd = config.weight_decay,\n",
        "        metrics=accuracy,\n",
        "        loss_func=config.loss_func, callback_fns=[partial(EarlyStoppingCallback, monitor='accuracy', min_delta=config.min_delta, patience=4)]\n",
        "      )\n",
        "    else:\n",
        "      learner = Learner(\n",
        "        databunch, model,\n",
        "        optimizer,\n",
        "        wd = config.weight_decay,\n",
        "        metrics=accuracy,\n",
        "        loss_func=config.loss_func,\n",
        "      )\n",
        "    \n",
        "    return learner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1KUoNpCq8fh"
      },
      "source": [
        "Actually create the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "da-sBmbOCLYc",
        "colab": {}
      },
      "source": [
        "# Create the classifier\n",
        "def create_classifier(config, df):\n",
        "  df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
        "  databunch = create_databunch(config, df_trn, df_valid)\n",
        "\n",
        "  return create_learner(config, databunch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgiSUldR9oHs",
        "colab_type": "text"
      },
      "source": [
        "Define train and test loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCa-FApr9puF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels):\n",
        "  classifier = create_classifier(config, df_train)\n",
        "  # Train the classifier\n",
        "  print(classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay))\n",
        "  # Predict\n",
        "  predictor = Predictor(classifier)\n",
        "  flat_predictions, flat_true_labels = [], []\n",
        "  column_index = df_eval.columns.get_loc(config_data.label_column)\n",
        "  for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
        "      class_text = row.RequirementText\n",
        "      class_label = row[column_index+1]\n",
        "      flat_true_labels.append(class_label)\n",
        "      prediction = predictor.predict(class_text)\n",
        "      flat_predictions.append(prediction)\n",
        "\n",
        "      log_text = '{}, {} -> {}'.format(class_text, label_indices.get(class_label), label_indices.get(prediction))\n",
        "      logLine(log_text)\n",
        "  \n",
        "  target_names = []\n",
        "  test_labels = unique_labels(flat_true_labels, flat_predictions)\n",
        "\n",
        "  test_labels = np.sort(test_labels)\n",
        "  for x in test_labels:\n",
        "    target_names.append(label_indices.get(x))\n",
        "\n",
        "  result = classification_report(flat_true_labels, flat_predictions, target_names=target_names, digits = 5)\n",
        "  logResult(result)\n",
        "  print(result)\n",
        "  overall_flat_predictions.extend(flat_predictions)\n",
        "  overall_flat_true_labels.extend(flat_true_labels)\n",
        "  return overall_flat_predictions, overall_flat_true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K15yO20N6w_u",
        "colab_type": "text"
      },
      "source": [
        "Decide how to fold and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqGyLEubBw60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overall_flat_predictions, overall_flat_true_labels = [], []\n",
        "initLog()\n",
        "if config.fold == Fold.TenFold:\n",
        "  skf = StratifiedKFold(n_splits=10)\n",
        "  fold_number = 1\n",
        "  for train, test in skf.split(df, df[config_data.label_column]):\n",
        "    df_train = df.iloc[train]\n",
        "    df_eval = df.iloc[test]\n",
        "    log_text = '/////////////////////// Fold: {} of {} /////////////////////////////'.format(fold_number,10)\n",
        "    logLine(log_text)\n",
        "    overall_flat_predictions, overall_flat_true_labels = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels)\n",
        "    fold_number = fold_number + 1\n",
        "elif config.fold == Fold.ProjFold:     \n",
        "  for k in config_data.project_fold:\n",
        "    test = df.loc[df['ProjectID'].isin(k)].index\n",
        "    train = df.loc[~df['ProjectID'].isin(k)].index\n",
        "    df_train = df.iloc[train]\n",
        "    df_eval = df.iloc[test]\n",
        "    log_text = '/////////////////////// Test-Projects: {} /////////////////////////////'.format(k)\n",
        "    logLine(log_text)\n",
        "    overall_flat_predictions, overall_flat_true_labels = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels)\n",
        "else:\n",
        "  df_train, df_eval = train_test_split(df,stratify=df[config_data.label_column], train_size=config.train_size, random_state= config.seed)\n",
        "  classifier = create_classifier(config, df_train)\n",
        "  # Train the classifier\n",
        "  print(classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay))\n",
        "  # Predict\n",
        "  predictor = Predictor(classifier)\n",
        "  column_index = df_eval.columns.get_loc(config_data.label_column)\n",
        "  print(column_index)\n",
        "  for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
        "      class_text = row.RequirementText\n",
        "      class_label = row[column_index+1]\n",
        "      overall_flat_true_labels.append(class_label)\n",
        "      prediction = predictor.predict(class_text)\n",
        "      overall_flat_predictions.append(prediction)\n",
        "\n",
        "      log_text = '{}, {} -> {}'.format(class_text, label_indices.get(class_label), label_indices.get(prediction))\n",
        "      logLine(log_text)\n",
        "\n",
        "\n",
        "target_names = []\n",
        "test_labels = df_eval[config_data.label_column].unique()\n",
        "\n",
        "test_labels = np.sort(test_labels)\n",
        "for x in test_labels:\n",
        "  target_names.append(label_indices.get(x))\n",
        "\n",
        "result = classification_report(overall_flat_true_labels, overall_flat_predictions, target_names=target_names, digits = 5)\n",
        "logResult(result)\n",
        "print(result)\n",
        "get_memory_usage_str()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeoCn0Bs-Wds",
        "colab_type": "text"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZxIqAcL9rjN"
      },
      "source": [
        "Save the model along with its config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXTWGILJ4kJx",
        "colab": {}
      },
      "source": [
        "def create_model_name():\n",
        "    name = 'NoRBERT_e{epochs}_{data_filename}'.format(epochs=str(config.epochs),data_filename=data_filenames[0][:-4])\n",
        "    return name\n",
        "\n",
        "def save_config(model_save_path, model_name):\n",
        "    settings = ''\n",
        "    for item in config.__dict__:\n",
        "        value = config[item]\n",
        "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
        "        settings += setting\n",
        "    save_path = model_save_path + model_name + '.config'\n",
        "    with open(save_path, 'w', encoding='utf-8') as out:\n",
        "        out.write(settings)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmbFM_7C9jp3",
        "colab": {}
      },
      "source": [
        "if save_model:\n",
        "    model_name = create_model_name()\n",
        "    model_save_path = config_data.root_folder + config_data.model_path\n",
        "    save_config(model_save_path, model_name)\n",
        "    model_save_file = model_save_path + model_name + '.pkl'\n",
        "    classifier.export(file = model_save_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}